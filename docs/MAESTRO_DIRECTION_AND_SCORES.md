## MAESTRO_DIRECTION_AND_SCORES.md
_Spec for Harmonyk Maestro scoring + "directional by design" doctrine._

Last updated: 2025-12-22

---

## 1. Core Doctrine

- Harmonyk is a **document-first operating system for founders and startups**.
- Harmonyk is **directional by design**:
  - Maestro doesn't just answer; it **points the user toward the next best move**.
  - Every important artifact and action is **scored from 0–100** so founders can iterate and improve.
- Product rule of thumb:
  - **≥ 90% → Ship it.**
  - **60–89% → Iterate / refine.**
  - **< 60% → Rethink / re-brief.**

This file defines how Maestro's scoring model should work across docs, tasks, and decisions, and how scores feed into the UI and future analytics.

---

## 2. Score Types (Global Model)

Maestro uses **0–100 scores** in a few distinct ways:

1. **Maestro Confidence %**
   - "Given what I currently know, how strongly do I back this suggestion or output?"
   - Stored per suggestion / generation / action.
   - Drives ranking of options and "Ready to ship" states.

2. **Clarity %**
   - "How well defined is the user's goal or brief for this decision/artifact?"
   - Starts low (e.g. 20–40%) when the user gives a vague ask.
   - Increases as the user answers Maestro's short, targeted follow-up questions.
   - Used to decide when Maestro is allowed to strongly recommend one option vs. keep things exploratory.

3. **User Rating %**
   - Editable field where the user rates the **realised quality or usefulness** of Maestro's output *after the fact*.
   - Stored per doc / task / decision and per Maestro suggestion where applicable.
   - Used to calibrate future Confidence % for that workspace and user.

4. **OS-Level KPIs (Workspace Scores)**
   - Aggregated 0–100 metrics computed periodically from events + entity scores:
     - **Execution %** – how consistently important tasks/doc trails are completed on time.
     - **Quality %** – average realised doc/task ratings.
     - **Momentum %** – how consistently deals/doc trails move through stages vs. stalling.
   - These power dashboard widgets and "state of your operating system" summaries.

---

## 3. Score Semantics and Thresholds

### 3.1 Standard bands for Maestro Confidence %

- **0–39% — Low confidence**
  - Maestro is guessing or data is very sparse.
  - UI should frame options as "exploratory" rather than "recommended".

- **40–69% — Medium confidence**
  - Maestro has some signal but is missing key parameters.
  - UI should nudge the user to answer 2–5 clarifying questions to increase Clarity % before acting.

- **70–89% — High confidence**
  - Maestro has enough data and prior outcomes for a strong recommendation.
  - UI can mark the option as "Preferred" but still show alternatives.

- **90–100% — "Ship it" band**
  - Maestro strongly believes the doc/decision is good enough for action given the user's stated goals.
  - Triggers "Over 90%? Ship it." guidance and "ready" states in the UI.

### 3.2 Product rule

- **≥ 90% → Ship it.**
  - In practice: encourage user to **send, sign, or save as record-of-truth** (when appropriate).
- **60–89% → Iterate / refine.**
  - Suggest one more iteration or targeted refinement on weak sections.
- **< 60% → Rethink / re-brief.**
  - Treat as "draft" or "misaligned brief" and prompt the user to sharpen goals and constraints.

These thresholds are defaults and can be refined per workspace based on historical performance.

---

## 4. Where Scores Attach (Entities)

### 4.1 Documents

For each `document` row (contracts, decks, white papers, patents, etc.):

- `maestro_confidence_pct` – current Maestro score for the doc's readiness.
- `maestro_clarity_pct` – how well the brief is defined for this doc/workflow.
- `user_score_pct` – latest user rating of the doc's quality for its intended purpose.
- `score_last_updated_at` – timestamp when any of these scores last changed.

Behavior:
- Document score updates on:
  - New draft generated by Maestro.
  - Significant edits applied (via Builder, Maestro suggestions, or Office sync).
  - User re-rates the artifact after sending/signing/outcome.

### 4.2 Tasks

For each `task` row:

- `maestro_confidence_pct` – how appropriate the task is given the current context/goals (for Maestro-created tasks).
- `user_score_pct` – user's retrospective rating of usefulness after completion.
- `planning_accuracy_pct` – system-calculated measure of whether the scheduled time aligned with actual completion (e.g. penalise repeated deferrals).

These support:
- Personal and team **Execution %** and planning behaviour insights.
- Future "re-scope your workload" recommendations.

### 4.3 Maestro Suggestions / Decisions

If there is a dedicated table for AI suggestions / actions (or in the `activity_log`):

- Store:
  - `maestro_confidence_pct`
  - `maestro_clarity_pct`
  - `user_score_pct` (if the user explicitly judges the suggestion)
  - Links to the associated entity (document, task, playbook run, etc.).

This gives a complete history of "what Maestro recommended, when, with what confidence, and how it turned out".

---

## 5. Iterative Narrowing Workflow

Maestro should treat most non-trivial actions (drafting important docs, making structural decisions) as **iterative narrowing**, not one-shot prompts:

1. **Initial pass**
   - Generate a draft or set of options quickly.
   - Set initial `maestro_confidence_pct` and low-to-medium `maestro_clarity_pct`.

2. **Clarifying questions**
   - Ask the user 2–5 targeted, mostly multiple-choice questions to raise `maestro_clarity_pct`.
   - After each answer, update option rankings and Confidence %.

3. **Refinement**
   - Apply refinements to weak sections (not the whole artifact) based on user feedback and ratings.

4. **Ship threshold**
   - Once Confidence % is ≥90 *and* Clarity % is above a configured threshold (e.g. 70–80):
     - Mark the doc/decision as "Ready".
     - Surface primary CTAs (send, sign, save as record-of-truth).
     - Show helper text: **"Over 90%? Ship it. Under 60%? Iterate / refine."**

5. **Outcome feedback**
   - After action (e.g. doc signed, deck sent, task completed):
     - Prompt user for `user_score_pct` on the doc or suggestion.
     - Log the outcome in the event log for future calibration.

---

## 6. Time, Events, and Calibration

### 6.1 Event log expectations

Maestro relies on a **strict, timestamped event log** (no "time is abstract" behaviour):

- Every major action should produce an immutable log entry:
  - `doc_created`, `doc_updated`, `doc_sent`, `doc_signed`, `doc_promoted_to_record_of_truth`
  - `task_created`, `task_rescheduled`, `task_completed`
  - `maestro_suggestion_created`, `maestro_suggestion_applied`, `maestro_suggestion_rejected`
- Each event must record:
  - `created_at` (server time, immutable)
  - `actor` (user / Maestro / integration)
  - `workspace_id` / `user_id`
  - Linked entity IDs and relevant score snapshot(s).

This gives Maestro a complete, time-accurate history to learn from.

### 6.2 Calibration loop

Periodic jobs (or online updates) should:

- Compare historical `maestro_confidence_pct` vs realised outcomes and `user_score_pct` for each class of decision (e.g. contract drafts, deck narratives, white papers, patents).
- Adjust internal calibration so that:
  - A "70%" confidence output turns out to be "good enough" roughly 70% of the time *for that workspace / persona*.
- Track simple calibration metrics per workspace:
  - e.g. `maestro_calibration_score_pct` – how well Maestro's confidence bands match reality.

This ensures that scores feel **honest and useful**, not arbitrary.

---

## 7. Office "Skin" Considerations (Forward-Looking)

For docs that are edited in Microsoft Word (or other Office apps):

- Harmonyk remains the **system of record** for:
  - Doc metadata and type.
  - Maestro scores.
  - Event history.
  - Links to deals/accounts/tasks.
- Office is treated as a **preferred editing surface**:
  - Harmonyk generates initial content and structure.
  - A "Build in Word" or "Open in Word" action stores the external file ID and mode.
  - "Sync latest from Word" pulls updates as new versions and triggers re-scoring.

Maestro's scoring model does **not** depend on the editing surface; it depends on:

- The doc's structure and content,
- The history of events and outcomes,
- User ratings and feedback.

---

## 8. DocSafe Integration Touchpoints (Forward-Looking)

When DocSafe is integrated (PGv2 and beyond):

- "Record-of-truth" documents with high Maestro scores and confirmed outcomes can be:
  - Stored in **DocSafe Drive** (secure, sharded storage).
  - Anchored via **DocSafe Sentinel** (hash + key metadata + timestamp on-chain as appropriate).
- Harmonyk remains responsible for:
  - Deciding when a doc is ready to be promoted.
  - Applying the "≥ 90% → Ship it" rule.
  - Tracking the full iterative history that led to the final, anchored version.

DocSafe is the long-term "forever" home for critical documents; Harmonyk is the place where they are created, iterated, and scored into existence.

